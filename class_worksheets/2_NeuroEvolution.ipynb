{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this worksheet, we'll start building a framework to EVOLVE BRAINS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#let's ignore overflow warnings from numpy in case we get\n",
    "#big numbers, no biggy.\n",
    "np.seterr(over='ignore', divide='raise')\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can make a generic neural network class that builds a random network with a specified number of inputs and outputs (and hidden nodes, but we'll get to that later!)\n",
    "\n",
    " This code takes care of randomly generating weights for each edge in our neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet():\n",
    "    #this is our squashing function\n",
    "    def activation_function(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    #this should look familiar, just a way to create a copy of \n",
    "    #the network without worying about pesky reference \n",
    "    def deepcopy(self):\n",
    "        new_net = SimpleNeuralNet(self.num_inputs, self.num_outputs, self.layer_node_counts)\n",
    "        new_net.layers = [np.copy(layer) for layer in self.layers]\n",
    "        return new_net\n",
    "    \n",
    "    #this is where the neural network does its computation!\n",
    "    def execute(self, input_vector):\n",
    "        # First we need to make sure we're getting the right\n",
    "        # number of inputs into our neural network.\n",
    "        assert len(input_vector) == self.num_inputs ,\\\n",
    "        \"wrong input vector size\"\n",
    "\n",
    "        # create a temporary variable to hold the values that should go\n",
    "        # into the next layer of the network.\n",
    "        # **at the start, this will just be our input**\n",
    "        next_v = input_vector\n",
    "\n",
    "        # iterate through layers, computing the activation\n",
    "        # of the weighted inputs from the previous layer\n",
    "        for layer in self.layers:\n",
    "            # add a bias to each layer [1]\n",
    "            next_v = np.append(next_v, 1)\n",
    "            \n",
    "            # pump the input vector through the matrix multiplication\n",
    "            # and our activation function\n",
    "            next_v = self.activation_function(np.dot(next_v, layer))\n",
    "            \n",
    "        return next_v\n",
    "        \n",
    "    def __init__(self, num_inputs, num_outputs, layer_node_counts=[]):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.layer_node_counts = layer_node_counts\n",
    "        self.num_outputs = num_outputs\n",
    "        self.layers = []\n",
    "        \n",
    "        last_num_neurons = self.num_inputs\n",
    "        for nc in layer_node_counts + [num_outputs]:\n",
    "            # for now, we'll just use random weights in the range [-5,5]\n",
    "            # +1 handles adding a bias node for each layer of nodes\n",
    "            self.layers.append(np.random.uniform(-5, 5, size=(last_num_neurons+1, nc)))\n",
    "            last_num_neurons = nc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have a class SimpleNeuralNet that will build a random neural network with `num_inputs` input nodes and `num_outputs` output nodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-3.18847777,  1.52866152],\n",
      "       [-1.72109833, -2.50564336],\n",
      "       [ 4.34490106,  2.56420479]])]\n"
     ]
    }
   ],
   "source": [
    "my_neural_net = SimpleNeuralNet(num_inputs=2, num_outputs=2)\n",
    "print(my_neural_net.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because this network could have multiple layers, the `layers` object is a *list of numpy arrays*. So, to get to the values of our input weights, we need to index into this larger collection of layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.18847777  1.52866152]\n",
      " [-1.72109833 -2.50564336]\n",
      " [ 4.34490106  2.56420479]]\n"
     ]
    }
   ],
   "source": [
    "print(my_neural_net.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we can do that, we can start to poke values in the neural network by changing their weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets try to tinker with this brain (muhahaha)\n",
      "\n",
      "I mutated the network!!\n",
      "[[-3.14159     1.52866152]\n",
      " [-1.72109833 -2.50564336]\n",
      " [ 4.34490106  2.56420479]]\n"
     ]
    }
   ],
   "source": [
    "print(\"lets try to tinker with this brain (muhahaha)\")\n",
    "my_neural_net.layers[0][0,0] = -3.14159\n",
    "print(\"\")\n",
    "print(\"I mutated the network!!\")\n",
    "print(my_neural_net.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how we've changed the element at `[0,0]` in the network to -3.14 by comparing this output to the print statement above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy arrays have a built in property named `shape` which will come in handy when you go to write a mutation function. It returns a `tuple` that holds the dimension of the array. \n",
    "For example, our 3x2 matrix we just mutated would return `(3,2)`. \n",
    "\n",
    "We can also index tuples to get individual dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = my_neural_net.layers[0].shape\n",
    "print(layer_dimensions)\n",
    "print(layer_dimensions[0])\n",
    "print(layer_dimensions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Like we discussed in class, we need to test our neural network against a range of inputs to get a good measurement of its performance. \n",
    "\n",
    "### Here I'll just define a set of inputs, and our expected output (in this case, the bit-mirroring output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_set = [[0,0],\n",
    "             [0,1],\n",
    "             [1,0],\n",
    "             [1,1]]\n",
    "\n",
    "output_set = [[0, 0], \n",
    "              [1, 0],\n",
    "              [0, 1], \n",
    "              [1, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can loop through each test input and execute our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing input:  [0, 0]\n",
      "output:  [0.98719335 0.92852203]\n",
      "target:  [0, 0]\n",
      "\n",
      "testing input:  [0, 1]\n",
      "output:  [0.93237786 0.51463618]\n",
      "target:  [1, 0]\n",
      "\n",
      "testing input:  [1, 0]\n",
      "output:  [0.76911328 0.9835827 ]\n",
      "target:  [0, 1]\n",
      "\n",
      "testing input:  [1, 1]\n",
      "output:  [0.37336979 0.83022503]\n",
      "target:  [1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test_index in range(len(input_set)):\n",
    "    test_output = my_neural_net.execute(input_set[test_index])\n",
    "    print(\"testing input: \", input_set[test_index])\n",
    "    print(\"output: \", test_output)\n",
    "    print(\"target: \", output_set[test_index])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But we actually want to quantitatively measure how close we are to the targets. We can just measure the distance between our target and actual output values and sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.079909441009588\n"
     ]
    }
   ],
   "source": [
    "total_distance = 0\n",
    "\n",
    "for test_index in range(len(input_set)):\n",
    "    test_output = my_neural_net.execute(input_set[test_index])\n",
    "    target_output = output_set[test_index]\n",
    "    \n",
    "    #typical distance formula between points\n",
    "    distances = np.sqrt((test_output - target_output)**2)\n",
    "    \n",
    "    #sum the distances up and keep a running tab!\n",
    "    total_distance += np.sum(distances)\n",
    "\n",
    "print(total_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's turn that calculation into a handy fitness function that expects an `input_set` and a `target_output_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_fitness(simple_net, input_set, target_output_set):\n",
    "    assert(len(input_set) == len(target_output_set))\n",
    "    total_distance = 0\n",
    "\n",
    "    for test_index in range(len(input_set)):\n",
    "        test_output = simple_net.execute(input_set[test_index])\n",
    "        target_output = output_set[test_index]\n",
    "\n",
    "        #typical distance formula between points\n",
    "        distances = np.linalg.norm(test_output - target_output)\n",
    "\n",
    "        #sum the distances up and keep a running tab!\n",
    "        total_distance += np.sum(distances)\n",
    "\n",
    "    #we actually want fitness to *increase* as things get fitter\n",
    "    #so we'll just negate this value.\n",
    "    return -total_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'll give you a little tournament selection function that expects the `population` of neural networks, an `input_set` and `target_set` to evaluate the networks with, and the `fit_func` that actually does the evaluation. \n",
    "\n",
    "# 1. But you'll have to write a function that implements mutations in the neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_selection(population, input_set, target_set, fit_func, tournament_size=3):\n",
    "    sample_pop = random.choices(population, k=tournament_size)\n",
    "    sample_pop_fitness = [fit_func(p, input_set, target_set) for p in sample_pop]\n",
    "    winner_idx = np.argmax(sample_pop_fitness)\n",
    "    \n",
    "    return sample_pop[winner_idx]\n",
    "    \n",
    "\n",
    "def mutate_network(simple_net, mutation_rate=0.001):\n",
    "    for layer_to_mut in simple_net.layers:\n",
    "        dims = layer_to_mut.shape\n",
    "        for weight_section in range(dims[0]):\n",
    "            for weight in range(dims[1]):\n",
    "                if random.random() < mutation_rate:\n",
    "                    layer_to_mut[weight_section][weight] += random.randint(-5, 5)\n",
    "    return simple_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code will work even without a mutation function, but you'll not be introducing any variation into the population. Once you have a working mutation function, you'll have more fun. \n",
    "\n",
    "## For this problem, we're going to try to evolve a solution to the last question from the last worksheet. \n",
    "| Input 1 | Input 2 | Output |\n",
    "|---------|---------|--------|\n",
    "| 0       | 0       | 0      |\n",
    "| 0       | 1       | 1      |\n",
    "| 1       | 0       | 1      |\n",
    "| 1       | 1       | 0      |\n",
    "\n",
    "\n",
    "## Recall that I said this one is tricky, and requires an additional *layer* in the neural network. Our SimpleNeuralNet class handles this easily for us!\n",
    "We just have to pass in a list of values, where each value is the number of nodes to include in a hidden layer. So, if we wanted 2 hidden layers, each with 5 neurons, we would build a SimpleNeuralNet and include `layer_node_counts=[5, 5]`. For this problem, we only need a single hidden layer, so we'll just pass in `[5]` for example to build a network with 5 neurons in one hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_set = [[0,0],\n",
    "             [0,1],\n",
    "             [1,0],\n",
    "             [1,1]]\n",
    "\n",
    "output_set = [[0], \n",
    "              [1],\n",
    "              [1], \n",
    "              [0]]\n",
    "\n",
    "\n",
    "pop_size = 100\n",
    "num_generations = 1000\n",
    "mutation_rate = 1E-3\n",
    "\n",
    "\n",
    "#Build up a population of SimpleNeuralNets\n",
    "#with one hidden layer made up of 5 neurons.\n",
    "population = [ SimpleNeuralNet(num_inputs=2, \n",
    "                               num_outputs=1, \n",
    "                               layer_node_counts=[5])\n",
    "              for i in range(pop_size)]\n",
    "\n",
    "\n",
    "for gen in range(num_generations):\n",
    "    clear_output()\n",
    "    display(gen)\n",
    "        \n",
    "    #do tournament selection to fill up the population\n",
    "    #with deepcopies of the neural networks. \n",
    "    selected_individuals = [tournament_selection(population, \n",
    "                                                 input_set, \n",
    "                                                 output_set, \n",
    "                                                 get_network_fitness).deepcopy()\n",
    "                            for _ in range(pop_size)]\n",
    "    #mutate them!\n",
    "    for individual in selected_individuals:\n",
    "        mutate_network(individual, mutation_rate)\n",
    "    \n",
    "    population = selected_individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Although we've been treating these inputs and outputs as binary values, neural networks are operating on continuous values and we can take advantage of that to visualize their output. We could also use this to make more sensitive fitness functions!\n",
    "# 2. Instead of plotting the individual at position `0`, change this code to plot the most fit individual from the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get 20 values between 0 and 1 for both x and y inputs\n",
    "x = y = np.linspace(0,1,20)\n",
    "\n",
    "#make a 20x20 matrix of all zeros\n",
    "mat_img = np.zeros((20, 20))\n",
    "population = sorted(population, key=lambda x: get_network_fitness(x, input_set, output_set))\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        mat_img[i,j] = population[0].execute([x[i],y[j]])[0]\n",
    "    \n",
    "pyplot.imshow(mat_img, cmap=pyplot.get_cmap(\"plasma\"))\n",
    "pyplot.colorbar()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing input:  [0, 0]\n",
      "output:  [0.]\n",
      "target:  [0]\n",
      "\n",
      "testing input:  [0, 1]\n",
      "output:  [1.]\n",
      "target:  [1]\n",
      "\n",
      "testing input:  [1, 0]\n",
      "output:  [1.]\n",
      "target:  [1]\n",
      "\n",
      "testing input:  [1, 1]\n",
      "output:  [0.]\n",
      "target:  [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test_index in range(len(input_set)):\n",
    "    test_output = population[0].execute(input_set[test_index])\n",
    "    print(\"testing input: \", input_set[test_index])\n",
    "    print(\"output: \", np.round(test_output))\n",
    "    print(\"target: \", output_set[test_index])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This problem is not trivial, and you may have to increase the number of generations to get a good solution. You could also try more creative fitness functions. It IS solvable with one hidden layer and 5 neurons, but it might be easier with more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
